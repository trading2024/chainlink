name: Find Flaky Tests

on:
  workflow_call:
    inputs:
      repoUrl:
        required: true
        type: string
        description: 'The URL of the repository to compare changes for detecting flaky tests.'
      projectPath:
        required: true
        type: string
        description: 'The path to the project to run the flaky test detection.'
        default: '.'  
      baseRef:
        required: true
        type: string
        description: 'The base reference or branch to compare changes for detecting flaky tests.'
      headRef:
        required: false
        type: string
        description: 'The head reference or branch to compare changes for detecting flaky tests. Default is the current branch.'
      runAllTests:
        required: false
        type: boolean
        description: 'Run all tests in the project.'
        default: false    
      runThreshold:
        required: false
        type: string
        description: 'The threshold for the number of times a test can fail before being considered flaky.'
        default: '0.9'
      findByTestFilesDiff:
        required: false
        type: boolean
        description: 'Find new or updated test packages by comparing test files diff.'
        default: true
      findByAffectedPackages:
        required: false
        type: boolean
        description: 'Find new or updated test packages by comparing affected packages.'
        default: true
      slackNotificationAfterTestsChannelId:
        description: "Slack channel ID to send the notification to for failed tests."
        required: false
        type: string
      extraArgs:
        required: false
        type: string
        default: '{}'
        description: 'JSON of extra arguments for the workflow.'
    secrets:
      SLACK_BOT_TOKEN:
        required: false

env:
  GIT_HEAD_REF: ${{ inputs.headRef || github.ref }}
  SKIPPED_TESTS: ${{ fromJson(inputs.extraArgs)['skipped_tests'] || '' }} # Comma separated list of test names to skip running in the flaky detector. Related issue: TT-1823
  DEFAULT_MAX_RUNNER_COUNT: ${{ fromJson(inputs.extraArgs)['default_max_runner_count'] || '8' }} # The default maximum number of GitHub runners to use for parallel test execution.
  ALL_TESTS_RUNNER_COUNT: ${{ fromJson(inputs.extraArgs)['all_tests_runner_count'] || '2' }} # The number of GitHub runners to use when running all tests `runAllTests=true`.
  TEST_REPEAT_COUNT: ${{ fromJson(inputs.extraArgs)['test_repeat_count'] || '5' }} # The number of times each runner should run a test to detect flaky tests.
  RUN_WITH_RACE: ${{ fromJson(inputs.extraArgs)['run_with_race'] || 'true' }} # Whether to run tests with -race flag.
  ALL_TESTS_RUNNER: ${{ fromJson(inputs.extraArgs)['all_tests_runner'] || 'ubuntu22.04-32cores-128GB' }} # The runner to use for running all tests.
  DEFAULT_RUNNER: 'ubuntu-latest' # The default runner to use for running tests.
  UPLOAD_ALL_TEST_RESULTS: ${{ fromJson(inputs.extraArgs)['upload_all_test_results'] || 'false' }} # Whether to upload all test results as artifacts.
  PRINT_FAILED_TESTS: ${{ fromJson(inputs.extraArgs)['print_failed_tests'] || 'false' }} # Whether to print failed tests in the GitHub console.
  MIN_PASS_RATIO: ${{ fromJson(inputs.extraArgs)['min_pass_ratio'] || '0.001' }} # The minimum pass ratio for a test to be considered as flaky. Used to distinguish between tests that are truly flaky (with inconsistent results) and those that are consistently failing. Set to 0 if you want to consider all failed tests as flaky.

jobs:
  get-tests:
    name: Get Tests To Run
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.split-packages.outputs.matrix }}
      workflow_id: ${{ steps.gen_id.outputs.workflow_id }}
      changed_test_files: ${{ steps.find-changed-test-files.outputs.test_files }}
      affected_test_packages: ${{ steps.get-tests.outputs.packages }}
      git_head_sha: ${{ steps.get_commit_sha.outputs.git_head_sha }}
      git_head_short_sha: ${{ steps.get_commit_sha.outputs.git_head_short_sha }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@9bb56186c3b09b4f86b1c65136769dd318469633 # v4.1.2
        with:
          fetch-depth: 0
          ref: ${{ env.GIT_HEAD_REF }}

      - name: Get commit SHA
        id: get_commit_sha
        run: |
          git_head_sha=$(git rev-parse HEAD)
          git_head_short_sha=$(git rev-parse --short HEAD)
          echo "git_head_sha=$git_head_sha" >> $GITHUB_OUTPUT
          echo "git_head_short_sha=$git_head_short_sha" >> $GITHUB_OUTPUT          

      - name: Set up Go 1.21.9
        uses: actions/setup-go@v5.0.2
        with:
          go-version: '1.21.9'
          cache: false

      - name: Install flakeguard
        shell: bash
        run: go install github.com/smartcontractkit/chainlink-testing-framework/tools/flakeguard@9da200418b01268201a0e6477832734a14720d07

      - name: Find new or updated test packages
        if: ${{ inputs.runAllTests == false }}
        id: get-tests
        shell: bash
        env:
          # Needed to run go test -list
          CL_DATABASE_URL: postgresql://postgres@localhost:5432/chainlink_test?sslmode=disable
        run: |
          PATH=$PATH:$(go env GOPATH)/bin
          export PATH

          PACKAGES=$(flakeguard find --find-by-test-files-diff=${{ inputs.findByTestFilesDiff }} --find-by-affected-packages=${{ inputs.findByAffectedPackages }} --base-ref=origin/${{ inputs.baseRef }} --project-path=${{ inputs.projectPath }})
          echo $PACKAGES
          echo "packages=$PACKAGES" >> $GITHUB_OUTPUT

      - name: Find changed test files 
        if: ${{ inputs.runAllTests == false }}
        id: find-changed-test-files
        shell: bash
        env:
          # Needed to run go test -list
          CL_DATABASE_URL: postgresql://postgres@localhost:5432/chainlink_test?sslmode=disable
        run: |
          PATH=$PATH:$(go env GOPATH)/bin
          export PATH

          TEST_FILES=$(flakeguard find --only-show-changed-test-files=true --base-ref=origin/${{ inputs.baseRef }} --project-path=${{ inputs.projectPath }})
          echo $TEST_FILES
          echo "test_files=$TEST_FILES" >> $GITHUB_OUTPUT
          
      - name: Split test packages into groups
        id: split-packages
        shell: bash
        run: |
          if [[ "${{ inputs.runAllTests }}" == "true" ]]; then
            # Use ALL_TESTS_RUNNER for a specified number of groups, each with "./..." to run all tests
            ALL_TESTS_RUNNER_COUNT=${{ env.ALL_TESTS_RUNNER_COUNT }}
            
            # Create the JSON array dynamically based on ALL_TESTS_RUNNER_COUNT
            json_groups=$(jq -nc --argjson count "$ALL_TESTS_RUNNER_COUNT" \
              '[range(0; $count) | { "testPackages": "./...", "runs_on": "'"${{ env.ALL_TESTS_RUNNER }}"'" }]')
              
            echo "$json_groups"
            echo "matrix<<EOF" >> $GITHUB_OUTPUT
            echo "$json_groups" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
            exit 0
          fi

          PACKAGES=(${{ steps.get-tests.outputs.packages }})
          DESIRED_GROUP_COUNT=$((${{ env.DEFAULT_MAX_RUNNER_COUNT }}))
          TOTAL_PACKAGES=${#PACKAGES[@]}

          # Number of groups should be no more than the number of packages
          MAX_GROUP_COUNT=$(($TOTAL_PACKAGES < $DESIRED_GROUP_COUNT ? $TOTAL_PACKAGES : $DESIRED_GROUP_COUNT))
          BASE_GROUP_SIZE=$(($TOTAL_PACKAGES / $MAX_GROUP_COUNT))
          EXTRA=$(($TOTAL_PACKAGES % $MAX_GROUP_COUNT))

          groups=()

          current_index=0
          for (( i=0; i < $MAX_GROUP_COUNT; i++ )); do
              # Determine the number of packages for the current group
              group_size=$BASE_GROUP_SIZE
              if [[ $i -lt $EXTRA ]]; then
                  group_size=$(($group_size + 1))
              fi
              
              # Extract the packages for the current group
              if [[ $group_size -gt 0 ]]; then
                  group=("${PACKAGES[@]:current_index:group_size}")
                  groups+=("{\"testPackages\":\"$(IFS=,; echo "${group[*]}")\", \"runs_on\":\"${{ env.DEFAULT_RUNNER }}\"}")
                  current_index=$(($current_index + $group_size))
              fi
          done

          # Convert groups array into a JSON array
          json_groups=$(printf '%s\n' "${groups[@]}" | jq -s .)
          echo "$json_groups"
          echo "matrix<<EOF" >> $GITHUB_OUTPUT
          echo "$json_groups" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Generate random workflow id
        id: gen_id
        shell: bash
        run: echo "workflow_id=$(uuidgen)" >> "$GITHUB_OUTPUT"          

  run-tests:
    name: Run Tests
    needs: get-tests
    runs-on: ${{ matrix.runs_on }}
    if: ${{ needs.get-tests.outputs.matrix != '' && needs.get-tests.outputs.matrix != '[]' }}
    timeout-minutes: 90
    strategy:
      fail-fast: false
      matrix: 
        include: ${{ fromJson(needs.get-tests.outputs.matrix) }}
    env:
      DB_URL: postgresql://postgres:postgres@localhost:5432/chainlink_test?sslmode=disable
    steps:
      - name: Checkout repository
        uses: actions/checkout@9bb56186c3b09b4f86b1c65136769dd318469633 # v4.1.2
        with:
          ref: ${{ env.GIT_HEAD_REF }}

      - name: Setup NodeJS
        uses: ./.github/actions/setup-nodejs
        with:
          prod: "true"
      - name: Setup Go
        uses: ./.github/actions/setup-go
        with:
          restore-build-cache-only: "true"
      - name: Setup Solana
        uses: ./.github/actions/setup-solana
      - name: Setup wasmd
        uses: ./.github/actions/setup-wasmd
      - name: Setup Postgres
        uses: ./.github/actions/setup-postgres
      - name: Touching core/web/assets/index.html
        run: mkdir -p core/web/assets && touch core/web/assets/index.html
      - name: Download Go vendor packages
        run: go mod download
      - name: Build binary
        run: go build -o chainlink.test .
      - name: Setup DB
        run: ./chainlink.test local db preparetest
        env:
          CL_DATABASE_URL: ${{ env.DB_URL }}        
      - name: Install LOOP Plugins
        run: |
          pushd $(go list -m -f "{{.Dir}}" github.com/smartcontractkit/chainlink-feeds)
          go install ./cmd/chainlink-feeds
          popd
          pushd $(go list -m -f "{{.Dir}}" github.com/smartcontractkit/chainlink-data-streams)
          go install ./mercury/cmd/chainlink-mercury
          popd
          pushd $(go list -m -f "{{.Dir}}" github.com/smartcontractkit/chainlink-solana)
          go install ./pkg/solana/cmd/chainlink-solana
          popd
          pushd $(go list -m -f "{{.Dir}}" github.com/smartcontractkit/chainlink-starknet/relayer)
          go install ./pkg/chainlink/cmd/chainlink-starknet
          popd

      - name: Go mod tidy
        shell: bash
        run: |
          cd ${{ inputs.projectPath }}
          go mod tidy

      - name: Generate random id
        id: gen_id
        run: echo "id=$(uuidgen)" >> "$GITHUB_OUTPUT"

      - name: Install flakeguard
        shell: bash
        run: go install github.com/smartcontractkit/chainlink-testing-framework/tools/flakeguard@9da200418b01268201a0e6477832734a14720d07

      - name: Run tests with flakeguard
        shell: bash
        run: flakeguard run --project-path=${{ inputs.projectPath }} --test-packages=${{ matrix.testPackages }} --run-count=${{ env.TEST_REPEAT_COUNT }} --min-pass-ratio=${{ env.MIN_PASS_RATIO }} --threshold=${{ inputs.runThreshold }} --race=${{ env.RUN_WITH_RACE }} --skip-tests=${{ env.SKIPPED_TESTS }} --print-failed-tests=${{ env.PRINT_FAILED_TESTS }} --output-json=test-result.json
        env:
          CL_DATABASE_URL: ${{ env.DB_URL }}

      - name: Upload test result as artifact
        if: always()
        uses: actions/upload-artifact@v4.4.3
        with:
          name: test-result-${{ needs.get-tests.outputs.workflow_id }}-${{ steps.gen_id.outputs.id }}
          path: test-result.json
          retention-days: 1   

  report:
    needs: [get-tests, run-tests]
    if: always()
    name: Report
    runs-on: ubuntu-latest
    outputs:
      test_results: ${{ steps.set_test_results.outputs.results }}
    steps:
      - name: Set Pretty Project Path
        id: set_project_path_pretty
        run: |
          if [ "${{ inputs.projectPath }}" = "." ]; then
            echo "path=github.com/${{ github.repository }}" >> $GITHUB_OUTPUT
          else
            echo "path=github.com/${{ github.repository }}/${{ inputs.projectPath }}" >> $GITHUB_OUTPUT
          fi

      - name: Download all test result artifacts
        uses: actions/download-artifact@v4.1.8
        with:
          path: test_results
          pattern:
            test-result-${{ needs.get-tests.outputs.workflow_id }}-*
            
      - name: Install flakeguard
        shell: bash
        run: go install github.com/smartcontractkit/chainlink-testing-framework/tools/flakeguard@9da200418b01268201a0e6477832734a14720d07
                
      - name: Set combined test results
        id: set_test_results
        shell: bash
        run: |
          set -e  # Exit immediately if a command exits with a non-zero status.
          if [ -d "test_results" ]; then
            cd test_results
            ls -R .

            # Fix flakeguard binary path
            PATH=$PATH:$(go env GOPATH)/bin
            export PATH

            # Use flakeguard aggregate-all to aggregate test results
            flakeguard aggregate-all --results-path . --output-results ../all_tests.json            

            # Count all tests
            ALL_TESTS_COUNT=$(jq 'length' ../all_tests.json)
            echo "All tests count: $ALL_TESTS_COUNT"
            echo "all_tests_count=$ALL_TESTS_COUNT" >> "$GITHUB_OUTPUT"

            # Use flakeguard aggregate-failed to filter and output failed tests based on PassRatio threshold
            flakeguard aggregate-failed --threshold "${{ inputs.runThreshold }}" --min-pass-ratio=${{ env.MIN_PASS_RATIO }} --results-path . --output-results ../failed_tests.json --output-logs ../failed_test_logs.json

            # Count failed tests
            if [ -f "../failed_tests.json" ]; then
              FAILED_TESTS_COUNT=$(jq 'length' ../failed_tests.json)
            else
              FAILED_TESTS_COUNT=0
            fi
            echo "Failed tests count: $FAILED_TESTS_COUNT"
            echo "failed_tests_count=$FAILED_TESTS_COUNT" >> "$GITHUB_OUTPUT"
          else
            echo "No test results directory found."
            echo "all_tests_count=0" >> "$GITHUB_OUTPUT"
            echo "failed_tests_count=0" >> "$GITHUB_OUTPUT"
          fi

      - name: Calculate Flakiness Threshold Percentage
        id: calculate_threshold
        run: |
          threshold_percentage=$(echo '${{ inputs.runThreshold }}' | awk '{printf "%.0f", $1 * 100}')
          echo "threshold_percentage=$threshold_percentage" >> $GITHUB_OUTPUT          

      - name: Upload Failed Test Results as Artifact
        if: ${{ fromJson(steps.set_test_results.outputs.failed_tests_count) > 0 }}
        uses: actions/upload-artifact@v4.4.3
        with:
          path: failed_tests.json
          name: failed-test-results.json
          retention-days: 7        

      - name: Upload Failed Test Logs as Artifact
        if: ${{ fromJson(steps.set_test_results.outputs.failed_tests_count) > 0 }}
        uses: actions/upload-artifact@v4.4.3
        with:
          path: failed_test_logs.json
          name: failed-test-logs.json
          retention-days: 7             

      - name: Upload All Test Results as Artifact
        if: ${{ fromJson(steps.set_test_results.outputs.all_tests_count) > 0 && env.UPLOAD_ALL_TEST_RESULTS == 'true' }}
        uses: actions/upload-artifact@v4.4.3
        with:
          path: all_tests.json
          name: all-test-results.json
          retention-days: 7

      - name: Create ASCII table with failed test results
        if: ${{ fromJson(steps.set_test_results.outputs.failed_tests_count) > 0 }}
        shell: bash
        run: |
          jq -r '["TestPackage", "TestName", "PassRatio", "RunCount", "Skipped"], ["---------", "---------", "---------", "---------", "---------"], (.[] | [.TestPackage, .TestName, .PassRatioPercentage, .Runs, .Skipped]) | @tsv' failed_tests.json | column -t -s$'\t' > failed_tests_ascii.txt
          cat failed_tests_ascii.txt

      - name: Create ASCII table with all test results
        if: ${{ fromJson(steps.set_test_results.outputs.all_tests_count) > 0 }}
        shell: bash
        run: |
          jq -r '["TestPackage", "TestName", "PassRatio", "RunCount", "Skipped"], ["---------", "---------", "---------", "---------", "---------"], (.[] | [.TestPackage, .TestName, .PassRatioPercentage, .Runs, .Skipped]) | @tsv' all_tests.json | column -t -s$'\t' > all_tests_ascii.txt
          cat all_tests_ascii.txt

      - name: Create GitHub Summary (General)
        run: |
          echo "## Flaky Test Detection Report for ${{ steps.set_project_path_pretty.outputs.path }} Project" >> $GITHUB_STEP_SUMMARY

      - name: Create GitHub Summary (Comparative Test Analysis)
        if: ${{ inputs.runAllTests == false }}
        run: |
          echo "### Comparative Test Analysis" >> $GITHUB_STEP_SUMMARY
          echo "Checked changes between \`${{ inputs.baseRef }}\` and \`${{ env.GIT_HEAD_REF }}\`. See all changes [here](${{ inputs.repoUrl }}/compare/${{ inputs.baseRef }}...${{ needs.get-tests.outputs.git_head_sha }}#files_bucket)." >> $GITHUB_STEP_SUMMARY

      - name: Create GitHub Summary (All Tests)
        if: ${{ inputs.runAllTests == 'true' }}
        run: |
          echo "### Running All Tests" >> $GITHUB_STEP_SUMMARY
          echo "All tests are being executed as \`runAllTests\` is set to true." >> $GITHUB_STEP_SUMMARY

      - name: Append Changed Test Files to GitHub Summary
        if: ${{ needs.get-tests.outputs.changed_test_files != '' && inputs.findByTestFilesDiff && !inputs.findByAffectedPackages }}
        run: |
          echo "### Changed Test Files" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          IFS=' ' read -ra ADDR <<< "${{ needs.get-tests.outputs.changed_test_files }}"
          for file in "${ADDR[@]}"; do
            echo "$file" >> $GITHUB_STEP_SUMMARY
          done
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Append Affected Test Packages to GitHub Summary
        if: ${{ needs.get-tests.outputs.affected_test_packages != '' }}
        run: |
          echo "### Affected Test Packages" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          IFS=' ' read -ra ADDR <<< "${{ needs.get-tests.outputs.affected_test_packages }}"
          for package in "${ADDR[@]}"; do
            echo "$package" >> $GITHUB_STEP_SUMMARY
          done
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Read Failed Tests File
        if: ${{ fromJson(steps.set_test_results.outputs.failed_tests_count) > 0 }}
        id: read_failed_tests
        run: |
          file_content=$(cat failed_tests_ascii.txt)
          echo "failed_tests_content<<EOF" >> $GITHUB_OUTPUT
          echo "$file_content" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Calculate Test Repeat Count
        id: calculate_test_repeat_count
        shell: bash
        run: |
          # Convert environment variables to integers
          ALL_TESTS_RUNNER_COUNT=${{ env.ALL_TESTS_RUNNER_COUNT }}
          TEST_REPEAT_COUNT=${{ env.TEST_REPEAT_COUNT }}

          # If runAllTests input is true, multiply the number of runners by the test repeat count as each runner runs all tests
          # Otherwise, use the test repeat count as each runner runs unique tests
          if [[ "${{ inputs.runAllTests }}" == "true" ]]; then
            test_repeat_count=$(( ALL_TESTS_RUNNER_COUNT * TEST_REPEAT_COUNT ))
          else
            test_repeat_count=$TEST_REPEAT_COUNT
          fi
          echo "test_repeat_count=$test_repeat_count" >> $GITHUB_OUTPUT

      - name: Append Flaky Tests to GitHub Summary
        if: ${{ fromJson(steps.set_test_results.outputs.failed_tests_count) > 0 }}
        run: |
          threshold_percentage=$(echo "${{ inputs.runThreshold }}" | awk '{printf "%.2f", $1 * 100}')
          min_pass_ratio_percentage=$(echo "${{ env.MIN_PASS_RATIO }}" | awk '{printf "%.2f", $1 * 100}')
          echo "### Flaky Tests :x:" >> $GITHUB_STEP_SUMMARY
          echo "Ran ${{ steps.set_test_results.outputs.all_tests_count }} unique tests ${{ steps.calculate_test_repeat_count.outputs.test_repeat_count }} times. Below are the tests identified as flaky, with a pass ratio lower than the ${threshold_percentage}% threshold:" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          cat failed_tests_ascii.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          echo "For detailed logs of the failed tests, please refer to the failed-test-results.json and failed-test-logs.json files in the Artifacts section at the bottom of the page. failed-test-logs.json contains all outputs from failed tests." >> $GITHUB_STEP_SUMMARY

      - name: Append Success Note if No Flaky Tests Found
        if: ${{ fromJson(steps.set_test_results.outputs.all_tests_count) > 0 && fromJson(steps.set_test_results.outputs.failed_tests_count) == 0 }}
        run: |
          echo "### No Flaky Tests Found! :white_check_mark:" >> $GITHUB_STEP_SUMMARY
          echo "Ran \`${{ steps.set_test_results.outputs.all_tests_count }}\` unique tests ${{ steps.calculate_test_repeat_count.outputs.test_repeat_count }} times and found no flakes." >> $GITHUB_STEP_SUMMARY

      - name: Append Additional Info to GitHub Summary
        if: ${{ fromJson(steps.set_test_results.outputs.all_tests_count) > 0 }}
        run: |
          echo "### Settings" >> $GITHUB_STEP_SUMMARY
          threshold_percentage=$(echo "${{ inputs.runThreshold }}" | awk '{printf "%.2f", $1 * 100}')
          min_pass_ratio_percentage=$(echo "${{ env.MIN_PASS_RATIO }}" | awk '{printf "%.2f", $1 * 100}')
          echo "| **Setting**             | **Value**  |" >> $GITHUB_STEP_SUMMARY
          echo "|-------------------------|------------|" >> $GITHUB_STEP_SUMMARY
          echo "| Go Project              | ${{ steps.set_project_path_pretty.outputs.path }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Minimum Pass Ratio      | ${min_pass_ratio_percentage}% |" >> $GITHUB_STEP_SUMMARY
          echo "| Flakiness Threshold     | ${threshold_percentage}%       |" >> $GITHUB_STEP_SUMMARY
          echo "| Test Run Count       | ${{ steps.calculate_test_repeat_count.outputs.test_repeat_count }}   |" >> $GITHUB_STEP_SUMMARY
          echo "| Race Detection          | ${{ env.RUN_WITH_RACE }}      |" >> $GITHUB_STEP_SUMMARY
          echo "| Excluded Tests          | ${{ env.SKIPPED_TESTS }}      |" >> $GITHUB_STEP_SUMMARY
          
      - name: Append No Tests Found Message to GitHub Summary
        if: ${{ fromJson(steps.set_test_results.outputs.all_tests_count) == 0 }}
        run: |
          echo "### No Tests To Execute" >> $GITHUB_STEP_SUMMARY
          echo "No updated or new Go tests found for ${{ steps.set_project_path_pretty.outputs.path }} project. The flaky detector will not run." >> $GITHUB_STEP_SUMMARY

      - name: Post comment on PR if flaky tests found
        if: ${{ fromJson(steps.set_test_results.outputs.failed_tests_count) > 0 && github.event_name == 'pull_request' }}
        uses: actions/github-script@v7
        env:
          MESSAGE_BODY_1: '### Flaky Test Detector for `${{ steps.set_project_path_pretty.outputs.path }}` project has failed :x:'
          MESSAGE_BODY_2: 'Ran new or updated tests between `${{ inputs.baseRef }}` and ${{ needs.get-tests.outputs.git_head_sha }} (`${{ env.GIT_HEAD_REF }}`).'
          MESSAGE_BODY_3: ${{ format('[View Flaky Detector Details]({0}/{1}/actions/runs/{2}) | [Compare Changes]({3}/compare/{4}...{5}#files_bucket)', github.server_url, github.repository, github.run_id, inputs.repoUrl, github.base_ref, needs.get-tests.outputs.git_head_sha) }}
          MESSAGE_BODY_4: '#### Flaky Tests'
          MESSAGE_BODY_5: 'Ran ${{ steps.set_test_results.outputs.all_tests_count }} unique tests. Below are the tests identified as flaky, with a pass ratio lower than the ${{ steps.calculate_threshold.outputs.threshold_percentage }}% threshold:'
          MESSAGE_BODY_6: '```'
          MESSAGE_BODY_7: '${{ steps.read_failed_tests.outputs.failed_tests_content }}'
          MESSAGE_BODY_8: '```'
        with:
          script: |
            const prNumber = context.payload.pull_request.number;

            const commentBody = `${process.env.MESSAGE_BODY_1}

            ${process.env.MESSAGE_BODY_2}

            ${process.env.MESSAGE_BODY_3}

            ${process.env.MESSAGE_BODY_4}

            ${process.env.MESSAGE_BODY_5}

            ${process.env.MESSAGE_BODY_6}
            ${process.env.MESSAGE_BODY_7}
            ${process.env.MESSAGE_BODY_8}`;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body: commentBody
            });

      - name: Send Slack message
        uses: slackapi/slack-github-action@6c661ce58804a1a20f6dc5fbee7f0381b469e001 # v1.25.0
        if: ${{ inputs.slackNotificationAfterTestsChannelId != '' && fromJson(steps.set_test_results.outputs.all_tests_count) > 0 }}
        id: slack
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
        with:
          channel-id: ${{ inputs.slackNotificationAfterTestsChannelId }}
          payload: |
            {
              "attachments": [
                {
                  "color": "${{ contains(join(needs.*.result, ','), 'failure') && '#C62828' || contains(join(needs.*.result, ','), 'cancelled') && '#FFA000' || '2E7D32' }}",
                  "blocks": [
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "Flaky Test Detector for `${{ steps.set_project_path_pretty.outputs.path }}` project - ${{ contains(join(needs.*.result, ','), 'failure') && 'Failed :x:' || contains(join(needs.*.result, ','), 'cancelled') && 'Was cancelled :warning:' || 'Passed :white_check_mark:' }}"
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "Ran changed tests between `${{ inputs.baseRef }}` and `${{ needs.get-tests.outputs.git_head_short_sha }}` (`${{ env.GIT_HEAD_REF }}`)."
                      }
                    },
                    {
                      "type": "section",
                      "text": {
                        "type": "mrkdwn",
                        "text": "${{ format('<{0}/{1}/actions/runs/{2}|View Flaky Detector Details> | <{3}/compare/{4}...{5}#files_bucket|Compare Changes>{6}', github.server_url, github.repository, github.run_id, inputs.repoUrl, inputs.baseRef, needs.get-tests.outputs.git_head_sha, github.event_name == 'pull_request' && format(' | <{0}|View PR>', github.event.pull_request.html_url) || '') }}"
                      }
                    }                 
                  ]
                }
              ]
            }
